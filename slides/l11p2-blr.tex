\documentclass[xcolor=x11names,compress,mathserif]{beamer}

\newcommand{\hackspace}{\hspace{4.2mm}}
\newcommand{\showstudent}[1]{}
\newcommand\hmmax{0}
\newcommand\bmmax{0}


\usepackage{../includes/MarkMathCmds}





% talk/author information
\newcommand{\authorname}{Mark van der Wilk}
\newcommand{\authoremail}{m.vdwilk@imperial.ac.uk}
\newcommand{\authoraffiliation}{
  Department of Computing\\Imperial
  College London}
\newcommand{\authortwitter}{markvanderwilk}
\newcommand{\slidesettitle}{\imperialBlue{Bayesian Linear Regression}}
\newcommand{\footertitle}{Measuring Generalisation}
\newcommand{\location}{Imperial College London}
\newcommand{\talkDate}{November 16, 2021}



\date{\imperialGray{\talkDate}}

% load defaults
\input{../includes/header.tex}


\input{../includes/titlepage.tex}
\linespread{1.2} 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Reference}
  \begin{center}
    \emph{Mathematics for Machine Learning:}\\[3mm]
    \emph{\url{https://mml-book.com}}\\[3mm]
    \cemph{Chapter 9}
  \end{center}
\end{frame}


\begin{frame}{Probabilistic Models}
\emph{Probabilistic model}: Model of the data is a probability distribution. \pause \\
\vspace{-0.4cm}
\begin{align*}
y_n = f(\vx_n; \vtheta) + \epsilon_n &&
\epsilon_n \sim \NormDist{0, \sigma^2}
\end{align*} \pause
We can now also estimate the \emph{unpredictability} of our problem:
  \begin{figure}
    \centering
    \includegraphics[width = 0.45\hsize]{./~figures/regression-noise.png}
  \end{figure}
\vspace{-0.3cm}
\begin{align}
(\vtheta^*, {\sigma^2}^*) = \argmax {}_{\vtheta, \sigma^2} \log p(\vy|\vtheta, \sigma^2, X)
\end{align}

Unpredictability remains even if we \emph{know} underlying function. \\
Goes by many names... e.g.~\emph{aleatoric uncertainty}. \pause
\end{frame}


\begin{frame}{Uncertainty in Parameters/Function}
Aren't we also uncertain when we have a lack of data?
  \begin{figure}
    \centering
    \includegraphics[width = 0.5\hsize]{./~figures/regression-uncertainty.png}
  \end{figure}
\vspace{-0.3cm}
This is uncertainty in the parameters that define the function! \\
Also goes by many names... e.g.~\emph{epistemic uncertainty}.
\end{frame}


\begin{frame}{Quantifying Uncertainty with Bayesian Inference}
If we knew that for a series of problems, our parameters $\vtheta$ were sampled from $p(\vtheta)$, then Bayes' rule would give us the probability distribution after observing our data $\vy$:
\begin{align}
\underbrace{p(\vtheta|\vy)}_{\text{posterior}} = \frac{\overbrace{p(\vy|\vtheta)}^{\text{likelihood}}\overbrace{p(\vtheta)}^{\text{prior}}}{\underbrace{p(\vy)}_{\text{evidence}}}
\end{align}

\begin{itemize}
\item Allows us to quantify uncertatinty in parameters $\vtheta$.
\item Bayesian inference makes a leap of faith: Choose a prior and assume this is the correct one.
\item Choosing priors is important $\implies$ Probabilistic Inference (Spring).
\end{itemize}
\end{frame}





\begin{frame}{Example}
  \begin{figure}
    \centering \includegraphics[width =
    0.48\hsize]{./~figures/demo_regression_posterior_with_mle_map_7}
    \hfill \includegraphics[width =
    0.48\hsize]{./~figures/demo_regression_posterior_samples_7}
  \end{figure}

  \begin{itemize}
  \item Light-gray: uncertainty due to noise \\(aleatoric uncertainty / unpredictability)
  \item Dark-gray: uncertainty due to parameter uncertainty \\ (epistemic uncertainty)
    \pause
  \item Right: Plausible functions under the parameter distribution
    (every single parameter setting describes one function)
  \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Distribution over Functions}
% Good thing: Bayesian linear regression induces a probability
% distribution over functions
Consider a linear regression setting 
\begin{align*}
y &= f(x) + \epsilon = a + bx + \epsilon\,,\quad
  \epsilon\sim\gauss{0}{\sigma_n^2}\\
  p(a,b) &= \gauss{\vec 0}{\mat I}
          \\
 \phantom{ [a_i, b_i] \sim p(a,b) }
\end{align*}
\begin{figure}
  \includegraphics[height = 5cm]{./~figures/parameter_prior}
\end{figure}
\end{frame}



\begin{frame}
\frametitle{Sampling from the Prior over Functions}
% Good thing: Bayesian linear regression induces a probability
% distribution over functions
Consider a linear regression setting 
\begin{align*}
y &=f(x) + \epsilon = a + bx + \epsilon\,,\quad
  \epsilon\sim\gauss{0}{\sigma_n^2}\\
p(a,b)& = \gauss{\vec 0}{\mat I}\\
  f_i(x) &= a_i + b_ix, \quad [a_i,b_i]\sim p(a,b)
\end{align*}


\onslide*<1>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/prior_samples_fct_distr_0}\end{figure}}
\onslide*<2>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/prior_samples_fct_distr_1}\end{figure}}
\onslide*<3>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/prior_samples_fct_distr_2}\end{figure}}
\onslide*<4>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/prior_samples_fct_distr_3}\end{figure}}
\onslide*<5>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/prior_samples_fct_distr_4}\end{figure}}
\onslide*<6>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/prior_samples_fct_distr_5}\end{figure}}
\onslide*<7>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/prior_samples_fct_distr_6}\end{figure}}
\onslide*<8>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/prior_samples_fct_distr_7}\end{figure}}
\onslide*<9>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/prior_samples_fct_distr_8}\end{figure}}
\onslide*<10>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/prior_samples_fct_distr_9}\end{figure}}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Sampling from the Posterior  over Functions}
% Good thing: Bayesian linear regression induces a probability
% distribution over functions
Consider a linear regression setting 
\begin{align*}
y &= f(x) + \epsilon =a + bx + \epsilon\,,\quad
  \epsilon\sim\gauss{0}{\sigma_n^2}\\
  p(a,b)& = \gauss{\vec 0}{\mat I}\\
  \mat X& = [x_1,\dotsc, x_N], ~ \vec y = [y_1,\dotsc, y_N]\quad  \text{ Training inputs/targets}
\end{align*}
\begin{figure}
\includegraphics[height = 5cm]{./~figures/training_data}
\end{figure}

\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}
%   \frametitle{Computing the Posterior}
% \begin{figure}
% \includegraphics[height = 2.5cm]{./~figures/training_data}
% \end{figure}

%   Define $\Phi(x) = \begin{bmatrix}
%     1\\
%     x
%     \end{bmatrix}$, $\vec\theta = \begin{bmatrix}a \\ b
%     \end{bmatrix}$.
%     \\ Then: $y = a + bx + \epsilon = \vec\theta\T \Phi(x) + \epsilon$.

% Training data $(x_1,y_1),\dotsc, (x_N, y_N)$ of inputs and observed targets.
    
%     Posterior:
%     \begin{align*}
% p(\vec\theta|\mat X, \vec y) &\propto~ p(\vec y|\mat X,
%       \vec\theta)p(\vec\theta) =  \gaussx{\vec y}{\vec\theta\T\Phi(\mat
%       X)}{\sigma_n^2\mat I}\gaussx{\vec\theta}{\vec 0}{\mat I}\\
%                              &=\gaussx{\vec\theta}{\vec m_N}{\mat S_N}\\
%       \mat S_N = (\mat I + \Phi(\mat X)\Phi(\mat X)\T/\sigma_n^2)\inv
%     \end{align*}
    
%   \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Sampling from the Posterior  over Functions}
% Good thing: Bayesian linear regression induces a probability
% distribution over functions
Consider a linear regression setting 
\begin{align*}
y &= f(x) + \epsilon =a + bx + \epsilon\,,\quad
  \epsilon\sim\gauss{0}{\sigma_n^2}\\
  p(a,b)& = \gauss{\vec 0}{\mat I}\\
  p(a,b|\mat X, \vec y) & =\gauss{\vec m_N}{\mat S_N} \qquad{\text{Posterior}}
\end{align*}
\begin{figure}
\includegraphics[height = 5cm]{./~figures/parameter_posterior}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Sampling from the Posterior  over Functions}
% Good thing: Bayesian linear regression induces a probability
% distribution over functions
Consider a linear regression setting 
\begin{align*}
y &= f(x) + \epsilon =a + bx + \epsilon\,,\quad
    \epsilon\sim\gauss{0}{\sigma_n^2}\\
   [a_i, b_i] &\sim p(a,b|\mat X, \vec y)\\
  f_i &= a_i + b_i x 
\end{align*}
\onslide*<1>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/posterior_samples_fct_distr_0}\end{figure}}
\onslide*<2>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/posterior_samples_fct_distr_1}\end{figure}}
\onslide*<3>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/posterior_samples_fct_distr_2}\end{figure}}
\onslide*<4>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/posterior_samples_fct_distr_3}\end{figure}}
\onslide*<5>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/posterior_samples_fct_distr_4}\end{figure}}
\onslide*<6>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/posterior_samples_fct_distr_5}\end{figure}}
\onslide*<7>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/posterior_samples_fct_distr_6}\end{figure}}
\onslide*<8>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/posterior_samples_fct_distr_7}\end{figure}}
\onslide*<9>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/posterior_samples_fct_distr_8}\end{figure}}
\onslide*<10>{\begin{figure}\includegraphics[height = 5cm]{./~figures/animation-param-func-dual/posterior_samples_fct_distr_9}\end{figure}}
\end{frame}



\begin{frame}{Model: Bayesian Linear Regression}
We never put a distribution on any $\vx_n$, so we drop from conditioning.
\begin{align}
p(\vtheta)  &= \NormDist{\vtheta; 0, \mathrm I_M} \\
p(y_n|\vtheta) &= \NormDist{y_n; \vphi(\vx_n)\transpose\vtheta, \sigma^2}
\end{align} \pause
\vspace{-0.4cm}
\begin{align}
p(\vy|\vtheta) = \NormDist{\vy; \Phi(\mat X)\vtheta, \sigma^2\mat I_N}
\end{align} \pause

Two goals:
\begin{itemize}
\item Find posterior over parameters $p(\vtheta|\vy)$
\item Find predictive posterior $p(\vy^*|\vy)$
\end{itemize}
\end{frame}

\begin{frame}{Posterior over Parameters}
Board:
\begin{itemize}
\item Equating coefficients (tests your matrix algebra skills!)
\item Joint Gaussian
\item Woodbury identity
\end{itemize}
\end{frame}

\begin{frame}{Method 1: Crunching densities}
\begin{align}
\log p(\vy|\vtheta) &= \log p(\vy|\vtheta) + \log p(\vtheta) \\
&= c - \frac{1}{2\sigma^2}(\vy - \Phi(\mat X)\vtheta)\transpose(\vy - \Phi(\mat X)\vtheta) - \frac{1}{2}\vtheta\transpose\vtheta
\end{align} \pause
This is a vector quadratic in $\vtheta$! {\tiny board} \pause $\implies$ Gaussian. \pause
\begin{itemize}
\item Equate coefficients. Can rearrange... \pause or find $\mathbb E$+$\mathbb V$ by other means \pause
\item Find maximum to find mean {\tiny board} \pause
\item Find Hessian to find covariance {\tiny board} \pause
\end{itemize}
\begin{align}
p(\vtheta|\vy) &= \mathcal{N} \Big(\vtheta; \left[\frac{1}{\sigma^2}\Phi(X)\transpose\Phi(X) + \mat I_M\right]\inv \frac{1}{\sigma^2}\Phi(X)\transpose\vy \\
&\qquad\qquad \left[\frac{1}{\sigma^2}\Phi(X)\transpose\Phi(X) + \mat I_M\right]\inv \Big)
\end{align}
\end{frame}

\begin{frame}{Method 2: Joint Gaussian}
Find
\begin{align}
p(\vtheta,\vy) = \NormDist{\begin{bmatrix}\vtheta \\ \vy\end{bmatrix}; \begin{bmatrix}\Exp{\vtheta}{\vtheta} \\ \Exp{\vy}{\vy}\end{bmatrix}, \begin{bmatrix}\Var{}{\vtheta} & \Cov{}{\theta,\vy} \\ \Cov{}{\vy,\vtheta} & \Var{}{\vy} \end{bmatrix}}
\end{align}

{\tiny board}

\begin{align}
p(\vtheta|\vy) = \mathcal{N}\Big(\vtheta; \Phi(X)\transpose\left[\Phi(X)\Phi(X)\transpose + \sigma^2 \mat I_N\right]\inv\vy, \nonumber \\
 \mat I_M - \Phi(X)\transpose\left[\Phi(X)\Phi(X)\transpose + \sigma^2 \mat I_N\right]\inv\Phi(X)\Big)
\end{align}
\end{frame}

\begin{frame}{Computational Considerations}
\end{frame}

\begin{frame}{Woodbury identity}
\end{frame}

\begin{frame}{Predictive posterior}
Board:
\begin{itemize}
\item Crunching densities
\item Equating coefficients (tests your matrix algebra skills!)
\item Joint Gaussian
\item Woodbury identity
\end{itemize}
\end{frame}

\begin{frame}{Method 1: Crunching densities}
First, how to express our target in terms of densities we know. \pause
\begin{align}
\onslide<2->{p(\vy^*|\vy) &\overset{\text{AT}}{=} \int \frac{p(\vy^*,\vtheta,\vy)}{p(\vy)} \mathrm d\vtheta} \\
\onslide<3->{&\overset{\text{MA}}{=} \int p(\vy^*|\vtheta)\frac{p(\vy|\vtheta)p(\vtheta)}{p(\vy)} \mathrm d\vtheta}
\end{align} \pause \pause
Next do the integrals / equating coefficients. \pause $\implies$ I recommend other method.
\end{frame}




\begin{frame}{Conclusion}
\end{frame}



% \begin{frame}{Gaussian Identities}
% \vspace{-0.3cm}
%   \begin{itemize}[<+->]
%   \item Linear transformations of Gaussians, are still Gaussian.
%   \item Joint Gaussian distribution
% \begin{align*}
%   p(\vec x,\vec y) = \gaussBig{
%   \begin{bmatrix}
% \colchar{$\vec\mu_x$}{blue}\\
% \colchar{$\vec\mu_y$}{red}
% \end{bmatrix}}{
% \begin{bmatrix}
% \colchar{$\mat\Sigma_{xx}$}{blue} & \colchar{$\mat\Sigma_{xy}$}{violet}\\
% \colchar{$\mat\Sigma_{yx}$}{violet} & \colchar{$\mat\Sigma_{yy}$}{red}
% \end{bmatrix}
% }
% \end{align*}
% \item \cemph{Marginal:} {\tiny proof using three principles from last lecture?}
% \begin{align*}
% p(\colchar{$\vec x$}{blue}) &= \int p(\colchar{$\vec x$}{blue},\colchar{$\vec y$}{red}) d\colchar{$\vec y$}{red}\\
% &= \gauss{\colchar{$\vec\mu_x$}{blue}}{\colchar{$\mat\Sigma_{xx}$}{blue}}
% \end{align*}
% \item \cemph{Conditional:} {\tiny proof using completing-the-square and block-inversion formula}
% \begin{align*}
%   p(\vec x|\vec y) &= \gauss{\vec\mu_{x|y}}{
% \mat\Sigma_{x|y}}\\
% \vec\mu_{x|y} &= \colchar{$\vec\mu_x$}{blue} + \colchar{$\mat\Sigma_{xy}$}{violet}\colchar{$\mat\Sigma_{yy}\inv$}{red}(\vec y - \colchar{$\vec\mu_y$}{red})\\
% \mat\Sigma_{x|y} &= \colchar{$\mat\Sigma_{xx}$}{blue} -\colchar{$\mat\Sigma_{xy}$}{violet}\colchar{$\mat\Sigma_{yy}\inv$}{red}\colchar{$\mat\Sigma_{yx}$}{violet}
% \end{align*}
% \end{itemize}

% \end{frame}






% \begin{frame}{MAP Estimation}
%   \begin{itemize}[<+->]
%     \item \cemph{Observation:} Parametric models that overfit tend to have
%       some extreme (large amplitude) parameter values
%     \item Mitigate the effect of overfitting by \cemph{placing a prior
%       distribution $p(\vec\theta)$ on the parameters} \\
%       \arrow Penalize extreme values that are implausible under that prior
%     \item Choose $\vec\theta^*$ as the parameter that \cemph{maximizes the (log)
%       parameter posterior}
%        $$\log p(\vec\theta|\mat X, \vec y) = \underbrace{\log p(\vec
%          y|\mat
%          X,\vec\theta)}_{\text{log-likelihood}}+\underbrace{\log
%          p(\vec\theta)}_{\text{log-prior}} + \text{ const}
%        $$
%      \item Log-prior induces a direct penalty on the
%        parameters
%      \item \emph{Maximum a posteriori estimate} (regularized least
%        squares)
%   \end{itemize}
  
  
% \end{frame}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{MAP Estimation (2)}
%   \begin{itemize}
%     \item Gaussian parameter prior $p(\vec\theta) = \gauss{\vec
%         0}{\alpha^2\mat I}$
%     \item Log-posterior distribution:
%       \begin{align*}
% \log p(\vec\theta|\mat X, \vec y) &= \colchar{$-\frac{1}{2\sigma^2} (\vec y-
%   \mat X\vec\theta)\T(\vec y - \mat X\vec\theta)$}{orange} \colchar{$-
%   \frac{1}{2\alpha^2}\vec\theta\T\vec\theta$}{blue} + \text{ const}\\
%  &= \colchar{$-\frac{1}{2\sigma^2}\|\vec y - \mat X \vec\theta\|^2$}{orange}\colchar{$-
%         \frac{1}{2\alpha^2}\|\vec\theta\|^2$}{blue} + \text{ const}
%       \end{align*}
%       \pause
% \item Compute gradient with respect to $\vec\theta$, set it to $\vec 0$\\
%   \arrow \emph{Maximum a posteriori estimate:}
%   $$
%   \colchar{$\vec\theta^{\text{MAP}} = (\mat X\T\mat X +\frac{\sigma^2}{\alpha^2}\mat I)\inv\mat
%     X\T\vec y$}{black}
%   $$
%   \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{Example: Polynomial Regression}

%   \begin{figure}
%     \centering
%     \onslide*<1>{
%     \includegraphics[width =
%     0.6\hsize]{./~figures/demo_regression_training_data}
%     \caption{Training data}
%   }%
%     \onslide*<2>{
%     \includegraphics[width =
%     0.6\hsize]{./~figures/demo_regression_mle_map_2}
%     \caption{2nd-order polynomial}
%   }%
%   \onslide*<3>{
%     \includegraphics[width =
%     0.6\hsize]{./~figures/demo_regression_mle_map_4}
%     \caption{4th-order polynomial}
%   }%
%   \onslide*<4>{
%     \includegraphics[width =
%     0.6\hsize]{./~figures/demo_regression_mle_map_6}
%     \caption{6th-order polynomial}
%   }%
%   \onslide*<5>{
%     \includegraphics[width =
%     0.6\hsize]{./~figures/demo_regression_mle_map_8}
%     \caption{8th-order polynomial}
%   }%
%   \onslide*<6>{
%     \includegraphics[width =
%     0.6\hsize]{./~figures/demo_regression_mle_map_10}
%     \caption{10th-order polynomial}
%   }%
% \end{figure}

% Mean prediction:
% $$
% \E[y_*|\vec x_*, \vec\theta_{\text{MAP}}^*] = \vec\phi(\vec x_*)\T\vec\theta_{\text{MAP}}^*
% $$

% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{Generalization Error}

%   \begin{figure}
%     \centering
%     \includegraphics[width = 0.5\hsize]{./~figures/demo_regression_test_RMSE}
%   \end{figure}


%   \begin{itemize}
%   \item MAP estimation ``delays'' the problem of overfitting
%   \end{itemize}

  
% \end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




% \nocite{Roberts2013, Krause2008, Deisenroth2015b, Deisenroth2009,Deisenroth2015, Calandra2014,Calandra2015a,
%   Calandra2014b,  Deisenroth2011c,
%   Quinonero-Candela2003a, Rasmussen2006, Sutton1998, Bertsekas2005,
%    Jones1998, Brochu2009, Osborne2009, Bertone2016, Baroukh2014,
%   Deisenroth2012d, Deisenroth2012,Frigola2013,Kocijan2004,Quinonero-Candela2005}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % REFERENCES
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[t,allowframebreaks]
% \frametitle{References}
% \linespread{1.0}
% \tiny
% \bibliographystyle{abbrv}
% \bibliography{/home/marc/research/svn/marc/literature/literature}
% \end{frame}



\end{document}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
